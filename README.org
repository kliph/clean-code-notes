* Notes on Clean Code
By Robert C. Martin
* Foreword
By James O. Coplien

Sensible, but offers 0 evidence.  Retreads the tired ground of appealing to Eastern mysticism to back up the sensible advice.
* Introduction

Ah yes, anecdotal evidence.  You'll have to work "/hard/"to learn this stuff.  Hopefully there will be some evidence to back these heuristics up.  Maybe?

But really though, case studies and these sorts of heuristics seem great.  Sensible approaches to writing software are great.  Surely if these sensible approaches are so profoundly useful there should be a way to measure their usefulness.

I feel a real appeal to machismo and toxic masculinity in this book, when it tells me this is "/hard work/" in contrast to a "feel good" book you can read on a plane.  I feel the appeal to machismo and toxic masculinity when it mentions that you'll gain a rich understanding of "principles, patterns, practices, and heuristics ... in your gut, fingers, and heart."  I could do without it.

I don't need a condescending tone to guilt-trip me into accepting what you say.  If it's sensible, show me how to do it and I will.  If it's measurable, demonstrate its effectiveness.

Finally, the WTFs per minute cartoon is great :D.

** A note on axioms

It would seem that Martin and company are attempting to do deductive reasoning from axioms.  My Mac's dictionary defines the word axiom as: "a statement or proposition which is regarded as being established, accepted, or self-evidently true."

They readily admit that other people may disagree with the claims that they treat as axiomatic.

I disagree in principle that there is any basis to reason axiomatically (that is, deductively) in the field of software development.  Even the assumption that computers are governed by the mathematics more than physics, barring all outside factors clearly being governed by empirical reality, falls apart with a modicum of scrutiny.  Processors in our machines are bound by the laws of physics.  Our software runs on the hard drives, RAM, and CPUs in these machines.  Humans write the software for, as Martin asserts, the intended audience of humans.

I posit that software development is governed by the rules of physics and psychology, and that we would do well to consider the empirical study of these fields when trying to determine Truths about software development.

The [[https://en.wikipedia.org/wiki/Scientific_method#Relationship_with_mathematics][scientific method]] serves as a means to test the validity of so called "axioms" that I believe would be better termed "folk psychology."  I find myself firmly in the [[https://en.wikipedia.org/wiki/Eliminative_materialism][school of thought espoused by Patty Churchland and others]] that seeks to question the basis for these "axioms," or common-sense understandings if you will, in order to determine the True nature of reality to the best of our abilities.

* Chapter 1: Clean Code
#+caption: There Will Be +Code+ Blood
[[https://media.giphy.com/media/tt3eTxBT0cgtW/giphy.gif]]

The doctors hand-washing anecdote argues in favor of strong validation via evidence.  It does not suggest that doctors have the prestige and privilege to buck the system, going against what their managers say in order to do what's best for their bosses (patients).

Grady Booch mentions that code should be readable.  I'm interested in that concept, specifically because I reckon that two programmers could have wildly different subjective definitions for what makes a certain bit of code readable.  I wonder whether there is any research determining the parameters that feed into this concept of readability.  This [[https://faculty.washington.edu/ajko/books/cooperative-software-development/comprehension.html][chapter from Andrew J. Ko]]
 seems like a good resource.

For what it's worth, I agree with Booch that clean code should read like prose.

In the next section (covering "Big" Dave Thomas), Martin asserts that "[t]here is, after all, a difference between code that is easy to read and code that is easy to change."  I see no evidence to support this dichotomy.  It certainly seems possible that readability and changeability are polar opposites, but it also seems possible that many overlapping factors contribute to what we (assuming we both hold similar referents) term readability and changeability.  We can't just take these assumptions for granted /a priori/.  We must investigate and interrogate these assumptions to learn whether they can tell us any meaningful information about the world.

Martin also points out that Thomas mentions twice that smaller code is better.  This brings to mind the some of the work in Adam Tornhill's /[[https://pragprog.com/book/atcrime/your-code-as-a-crime-scene][Your Code as a Crime Scene]]/.  I should revisit that when I can find the time.  It would seem that there is an argument to be made in favor of smaller code bases given constant rates of errors (bugs).

Ron Jeffries makes an interesting point on refactoring to extract methods: "If it's a method, [refactor it], resulting in one method that says more clearly *what* it does, and some submethods saying *how* it is done." Bold indicates my emphasis.

Interesting that Martin's well-known programmers are all white men.  Might be interesting to ask a more diverse sample to share their opinions on what makes code "clean".  I don't presume that the findings would be too different, but one can't know without conducting the research.

Okay, so, comparing Martin's "School of Thought" to a martial arts tradition is a total cop out.  It's not as if martial arts traditions [[https://www.ncbi.nlm.nih.gov/pubmed/31191109][haven't]] [[https://www.ncbi.nlm.nih.gov/pubmed/31240587][been]] [[https://www.ncbi.nlm.nih.gov/pubmed/31373295][rigorously]] [[https://www.ncbi.nlm.nih.gov/pubmed/31343555][studied]] [[https://www.ncbi.nlm.nih.gov/pubmed/31336837][and]] [[https://www.ncbi.nlm.nih.gov/pubmed/31282402][tested]] [[https://www.ncbi.nlm.nih.gov/pubmed/31261524][for]] [[https://www.ncbi.nlm.nih.gov/pubmed/30846917][effectiveness]] (and I couldn't resist a [[https://www.ncbi.nlm.nih.gov/pubmed/30832454][couple]] [[https://www.ncbi.nlm.nih.gov/pubmed/30694967][more]]).  Why not do the same for these ideas that, at this point, "/are/ absolutes"?

[[https://media.giphy.com/media/sJiAhV5VPheTK/giphy.gif]]

#+BEGIN_QUOTE
In the 80s and 90s we had editors like Emacs that kept track of every keystroke.
#+END_QUOTE

LOL that's *rich*!  See [[https://en.wikipedia.org/wiki/Emacs#cite_note-1][these]] [[https://en.wikipedia.org/wiki/Emacs#cite_note-jwz_timeline-2][citations]] for more information.
* Chapter 2: Meaningful Names
by Tim Ottinger

Nice, choose names that communicate intent.  Seems reminiscent of Zach Tellman's /[[https://leanpub.com/elementsofclojure][Elements of Clojure]]/ and Bozhidar Batsov's [[https://guide.clojure.style/][Clojure Style Guide]] (not to mention, although pausing specifically to mention, Batsov's work on the [[https://rubystyle.guide/][Ruby Style Guide]]).

In the Make Meaningful Distinctions section, Ottinger mentions something I found very puzzling initially:

#+BEGIN_QUOTE
For example, because you can't use the same name to refer to two different things in the same scope, you might be tempted to change one name in an arbitrary way.  Sometimes this is done by misspelling one, leading to the surprising situation where correcting spelling errors leads to an inability to compile.
#+END_QUOTE

I've definitely encountered this!

In a footnote, Ottinger mentions using =klass= because =class= was used elsewhere.  See also:

#+BEGIN_SRC js
  var that = this;
#+END_SRC

I can certainly see why it would be necessary.  But still.  Yuck.

[[https://media.giphy.com/media/ZF32u4EqI0AHqUQucs/giphy.gif]]

"/The length of a name should correspond to the size of its scope./"  This is a nice heuristic and one I use myself.

Here's a hypothesis.  Seems like testing comprehension by varying name length by objective scope would be a relatively straightforward experiment.  Conditions could be "long names" for large scope / "short names" for small scope, "short names" for large scope / "long names" for small scope, random length names for all scopes, etc.

Not sure I buy the argument regarding Interfaces and Implementations.  Those seem to be cases where you have a meaningful distinction between multiple bits of code dealing with the same referent.  Ottinger seems to think it is necessary to use encodings for these cases, but using =I= as a prefix is "a distraction at best and too much information at worst."  =AbstractFoo=, =IFoo=, and =FooImpl= all make sense to me.

I'm totally here for picking one word per concept.  That's a practice I use all the time.

[[https://media.giphy.com/media/NEvPzZ8bd1V4Y/giphy.gif]]

Regarding the assumption that everyone who reads your code will be programmers, I say, "Uhhhh, not so sure that's always the case, but granted."  When Ottinger recommends using "computer science ... terms, algorithm names, pattern names, path terms and so forth."  I [[https://twitter.com/elementsofclj/status/1074710832938971136][disagree]].

It's incumbent on the author to define the terms for the reader.  I have often accomplished this by including a Glossary defining terms of art in a project.  This goes for domain-specific terms as well as general purpose terms which may or may not be used in novel ways to solve the problem at hand.

[[https://media.giphy.com/media/P5wPrhzZDdeJW/giphy.gif]]
* Chapter 3: Functions
Keep 'em tiny.

[[https://media.giphy.com/media/cOVtfbB28ejn0lk3sq/giphy.gif]]

There's a nice principle here that a function is doing one and only one thing if and only if you can no longer meaningfully refactor it into a function that is not merely a restatement of its implementation.  But who is to say that you and I agree on what meaningful refactoring are?  It would be helpful if there were some more objective way to define this.

I can see how the code in Listing 3-7 is refactored to make the functions as smol as possible.  And I can also see how nicely the logic and level of abstraction flows from function to function.  I like how each function introduces the next.  However, I don't see how this approach is objectively better than a more gestalt-oriented approach.

I'm reminded of [[https://neuro.wisc.edu/staff/jones-mathew-2/][Matt Jones]] teaching me about "lumpers" and "splitters" in science.  Lumpers tend to try to group similar ideas into a cohesive gestalt in order to gain some explanatory power through the agglomerated whole.  Splitters tend to try to gain explanatory power by breaking things down into their atomic parts and understanding the similarities among things that share similar parts.

The approach Martin is advocating for here seems to fall squarely in the splitter category.  I remain unconvinced, though, that the humans who are doing the programming can easily handle (today and two weeks from now) a class with, by my count, 5 pieces of internal state, 2 public methods, and 16 private methods.  I grant, as Martin mentions, that tooling can help with this.  Indeed, I leverage tooling to help with my poor recall all the time.  However, I would argue that the tradeoffs associated with breaking a single method into 23 things becomes a question of [[https://en.m.wikipedia.org/wiki/Memory_span][human memory span]], rather than platonic elegance.

[[https://media.giphy.com/media/PcfozPlZSzARO/giphy.gif]]

Also let me get in a jab that =SetupTeardownIncluder.java= is not a super descriptive name!  Maybe it's useful in context.

I'm reminded of inheriting some code responsible for plotting some data.  It had been factored down to such small functions that it was hard to follow what it was doing at all.  Each function did one thing, and the sum of those functions did generate the plots as intended.  But figuring out how to extend the functionality was extremely difficult due to the distribution of level of abstraction across all of these functions.  In my opinion an easier way to read that code was that a single function was responsible for generating a single graph and may call out to multiple DRYed out helper functions that could live at lower levels of abstraction.  I refactored it to allow that organization and I felt that that made the code more extensible.

Most of the recommendations in this chapter strike me as testable hypotheses rather than unassailable imperatives to be ignored at your own peril.  That being said, they seem sensible enough rules of thumb to follow.

Also, I see function arguments as an area where "modern IDEs", what I call tooling, can help.

* Chapter 4: Comments

Man you could write a thesis on the depiction of women in this book.

[[https://media.giphy.com/media/31UDILmnnQOjnKKoKB/giphy.gif]]

WTF is up with associating women with comments, you know the thing you're not supposed to do?

[[https://media.giphy.com/media/QjIz1AqkGTszK/giphy.gif]]

WTF is up with associating +a woman+ women (just got to the scolding mother in this chapter) with motherhood?

[[https://media.giphy.com/media/3ohzdOKC7b6VbCPGQo/giphy.gif]]

WTF is up with not asking any women about their definitions of clean code?

[[https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Commodore_Grace_M._Hopper%2C_USN_%28covered%29.jpg/1536px-Commodore_Grace_M._Hopper%2C_USN_%28covered%29.jpg]]

Martin seems to be crying out for [[https://en.m.wikipedia.org/wiki/Literate_programming][literate programming]] to help keep comments current, that is to say code-driven (and, in homoiconic languages, therefore, data-driven).

The failure to keep comments current and accurate is not a given.  Surely, just as clean code requires gumption on the part of the author, comments require gumption on the part of the author to maintain their accuracy and integrity.  Why then, on the subject of comments, is Martin comfortable with throwing in the towel?

I just don't get why clean comments aren't important.  Martin's thesis seems to be: All comments are bad except for a few, and you should feel bad if you write them.

[[https://media.giphy.com/media/VeB9ieebylsaN5Jw8p/giphy.gif]]

How is that productive?
* Chapter 5: Formatting
[[https://media.giphy.com/media/SF4aJKqEchIiY/giphy.gif]]

Set rules and automate their enforcement.

* Chapter 6: Objects and Data Structures

[[https://media.giphy.com/media/Ws9uuyPnZhUOSR8tjx/giphy.gif]]

The recommendations for abstraction are good, but they aren't the only way to work around these problems of leaky scope and coupling.

[[https://media.giphy.com/media/rEMU7AjqjvMLC/giphy.gif]]

For example, have a gander at this Rich Hickey [[https://www.youtube.com/watch?v=YR5WdGrpoug][talk]] ([[https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/MaybeNot.md][transcript here]]).

* Chapter 7: Error Handling
By Michael Feathers
